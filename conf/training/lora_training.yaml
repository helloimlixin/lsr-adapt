# @package _group_

# Training configuration for LoRA fine-tuning
evaluation_strategy: "epoch"  # Evaluate at the end of each epoch
save_strategy: "epoch"  # Save at the end of each epoch
num_train_epochs: 20  # Total number of training epochs
learning_rate: 5e-4  # Typically higher than full fine-tuning
weight_decay: 0.01  # L2 regularization
per_device_train_batch_size: 256  # Can be larger with LoRA
per_device_eval_batch_size: 256
logging_steps: 50  # How often to log metrics
save_total_limit: 3  # Only keep the 3 most recent checkpoints
metric_for_best_model: "accuracy"  # Which metric to use for best model selection
load_best_model_at_end: true  # Load the best model at the end of training
fp16: true  # Use mixed precision training (much faster with minimal accuracy impact)
gradient_accumulation_steps: 1  # Accumulate gradients over multiple steps